{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<a target=\"_blank\" href=\"http://eeit.sec.gob.mx/\">\n",
        "  <img src=\"http://eeit.sec.gob.mx/assets/media/others/bubble-12.png\" width=250pt style=\"padding-bottom:5px;\" />\n",
        "</a>\n",
        "<br/><br/><br/>\n",
        "<a target=\"_blank\" href=\"https://educacion.sonora.gob.mx/\">\n",
        "  <img src=\"https://educacion.sonora.gob.mx/images/2023/07/17/logo-sec.svg\" width=150pt style=\"padding-bottom:5px;\" />\n",
        "</a>\n",
        "</center>\n",
        "\n",
        "<h1><b>Transferencia de aprendizaje en reconocimiento de imágenes</b> </h1>\n",
        "\n",
        "<author>Julio Waissman Vilanova</author>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/juliowaissman/eeit2024/blob/main/transfer.ipynb\">\n",
        "<img src=\"https://i.ibb.co/2P3SLwK/colab.png\" width=30pt />\n",
        "<i>Para usar en Google Colab</i></a>"
      ],
      "metadata": {
        "id": "TFi2S_DEAEC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "En esta libreta vamos a presentar la aplicación de transferencia de aprendizaje para clasificación de imágenes.\n",
        "Esta es la base para buscar otros modelos y otras aplicaciones posibles."
      ],
      "metadata": {
        "id": "5ZWUqutNAheC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ww7p6lP7ACzw"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "# Las que no pueden faltar\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Para obtener los modelos preentrenados\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta libreta muestra cómo reutilizar un modelo existente  usando como ejemplo la clasificación de cinco especies de flores. Se va a utilizar un modelo entrenado en el conjunto de datos ImageNet y guardado en [TensorFlow Hub](https://www.tensorflow.org/hub).\n",
        "\n",
        "Se van a mantener las capas de extracción de características y unicamente se va a modificar el clasificador.\n",
        "\n",
        "Opcionalmente, si se tiene tiempo y capacidad de cómputo, esposible usar un ajuste fino de los pesos del extractor de características.\n",
        "\n",
        "Este ejemplo está tomado de [este ejemplo de la documentación de tensorflow](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining)."
      ],
      "metadata": {
        "id": "aEri_3W8BR34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obteniendo las imágenes del conjunto de entrenamiento y prueba\n",
        "\n",
        "Vamos a descargar las imágenes y guardarlas en un directorio dentro de nuestro espacio de trabajo (al finalizar la sesión se borrarán si se ejecuta en *Colab*)."
      ],
      "metadata": {
        "id": "alry_4bkC78z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True\n",
        ")"
      ],
      "metadata": {
        "id": "x03ejRbODE17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Y que tiene lo que acabamos de descargar y donde lo descargamos? Veamos:"
      ],
      "metadata": {
        "id": "PZZluyQUFeRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ~/.keras/datasets/flower_photos/"
      ],
      "metadata": {
        "id": "vlItWn2RE5nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección de un modelo preentrenado\n",
        "\n",
        "Vamos ahora a seleccionar un modelo a descargar. Para esto vamos a poner una lista (tomada obviamente del tutorial desarrollado por el equipo de TensorFlow Hub) de los modelos disponibles, así como el tamaño de las imágenes que usa cada uno de los modelos. Los vamos a guardar en un diccionario:"
      ],
      "metadata": {
        "id": "8WHMZug0Fqay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300,\n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "print('Los modelos que podemos escoger son los siguientes:')\n",
        "\n",
        "print('\\n'.join(model_handle_map.keys()))"
      ],
      "metadata": {
        "id": "tfPndHgPFvCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y ahora si, seleccionamos nuestro modelo, el número de pixeles que maneja de imágenes de entrada y el tamaño del minibatch para reentrenar"
      ],
      "metadata": {
        "id": "lGAv6ljeHA8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"efficientnetv2-xl-21k\"\n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Modelo seleccionado: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Tamaño de la imagen: {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "1oduH5XBHB6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flujo de trabajo y preprocesamiento de datos\n",
        "\n",
        "Ahora vamos a procesar los datos, y para eso vamos a usar el [modulo ``tf.data`](https://www.tensorflow.org/guide/data?hl=en) que permite crear *pipelines* para consumir (potencialmente) grandes cantidades de datos.\n",
        "\n",
        "Vamos a seleccionar un conjunto de entrenamiento con el 80% de los datos. Se separan los valores de salida y se generan minibatches de tamaño `BATCH_SIZE``."
      ],
      "metadata": {
        "id": "sbumwOnpLUw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.20,\n",
        "  subset=\"training\",\n",
        "  label_mode=\"categorical\",\n",
        "  seed=123, # Para asgurar la reproducibilidad de los conjuntos de datos\n",
        "  image_size=IMAGE_SIZE,\n",
        "  batch_size=1 # Un dato por batch solo para sacar el tamaño de la muestra\n",
        ")\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "\n",
        "class_names = tuple(train_ds.class_names)\n",
        "\n",
        "# Ahora si, hacenmos batches de tamaño BATCH_SIZE\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "train_ds = train_ds.repeat()"
      ],
      "metadata": {
        "id": "3hhffrxPLlbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y ahora el conjunto de validación"
      ],
      "metadata": {
        "id": "RNRESHhFMJzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  label_mode=\"categorical\",\n",
        "  seed=123,\n",
        "  image_size=IMAGE_SIZE,\n",
        "  batch_size=1\n",
        ")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "BnzKqUPrMZYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como unico paso de preprocesamiento, vamos a reescalar (normalizar) el valor de las imágenes. Recuerda que los valores de los canales RGB de cada pixel están dados desde 0 hasta 255 (8 bits, $2^8 = 256 valores diferentes)."
      ],
      "metadata": {
        "id": "WOGIflGFMiGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    lambda images, labels: (preprocessing_model(images), labels)\n",
        ")\n",
        "val_ds = val_ds.map(\n",
        "    lambda images, labels: (normalization_layer(images), labels)\n",
        ")"
      ],
      "metadata": {
        "id": "XF5ZJbr_M3U3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diseño del modelo de aprendizaje\n",
        "\n",
        "Vamos a rediseñar el modelo de aprendizaje a partir del modelo descargado. Lo que vamos a hacer es sustituir la última capa del modelo. Para un modelo entrenado con el conjunto de datos de [ImageNet](https://www.image-net.org/) es una capa tipo *softmax* con 1,000 valores diferentes (clasifica entre 1,000 categorías). Esta la vamos a sustituir por un problema mñas modesto de aprender entre 5 clases de flores diferentes."
      ],
      "metadata": {
        "id": "gUrochThNuW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "do_fine_tuning = False\n",
        "\n",
        "print(\"Modelo basado en \", model_handle)\n",
        "print(\"¿Vamos a hacer ajuste fino? \", do_fine_tuning)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  # Define explicitamente el input shape\n",
        "  tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "\n",
        "  # Carga el modelo preentrenado, con o sin fine tunning\n",
        "  hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "\n",
        "  # Dropout para ayudar a la generalización\n",
        "  tf.keras.layers.Dropout(rate=0.2),\n",
        "\n",
        "  # Capa de salida lineal del número de clases y con regularización\n",
        "  tf.keras.layers.Dense(\n",
        "      len(class_names),\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
        "  )\n",
        "  ],\n",
        "  name='Mi-modelo-bien-bonito'\n",
        ")\n",
        "\n",
        "# Construye el modelo haciendo explicito el tamaño de entrada\n",
        "model.build((None,) + IMAGE_SIZE + (3,))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jnPVUQHQOihF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compilar, entrenar y validar\n",
        "\n",
        "Ahora compilamos y decidimos que optimizador, función de pérdida y métricas utilizar."
      ],
      "metadata": {
        "id": "ML2SEJHLPkNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "WzQbF417PsVo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps\n",
        ").history"
      ],
      "metadata": {
        "id": "idEzt-ueP_hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y ahora grafiquemos la función de pérdida y la métrica seleccionada por los *epochs* que escogimos simular."
      ],
      "metadata": {
        "id": "4NUILXfZQUQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist[\"loss\"], label='Entrenamiento')\n",
        "plt.plot(hist[\"val_loss\"], label='Validación')\n",
        "plt.ylabel(\"Funcion de pérdida\")\n",
        "plt.xlabel(\"Pasos de aprendizaje\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist[\"accuracy\"], label='Entrenamiento')\n",
        "plt.plot(hist[\"val_accuracy\"], label='Validación')\n",
        "plt.ylabel(\"% de error\")\n",
        "plt.xlabel(\"Pasos de aprendizaje\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "48HZdZkWQPd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso del modelo reentrenado\n",
        "\n",
        "Probemos como se enviaría una imagen nueva al clasificador:"
      ],
      "metadata": {
        "id": "3RwOiGNiRng8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Con el iterador se obtiene un minibatch de imágenes y clases\n",
        "x, y = next(iter(val_ds))\n",
        "\n",
        "# Tomemos la primer imagen y su indice\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "\n",
        "# Mostramos la imagen\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expande la imagen a (1, 224, 224, 3) para introducirla al modelo\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "\n",
        "# Obten el indice con mayor valor en la capa de salida\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])"
      ],
      "metadata": {
        "id": "1amTZTvrRq-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y si el modelo te gustó y lo quieres usar más adelate, sólamente hay que guardarlo."
      ],
      "metadata": {
        "id": "aSAnuxyRRyxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "metadata": {
        "id": "GHen5LQzR6EV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /tmp"
      ],
      "metadata": {
        "id": "PJWdEp4rbgQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}