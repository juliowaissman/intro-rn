{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42upscu8IOhB"
      },
      "source": [
        "<h1><b>Mi primera red neuronal en TensorFlow</b> </h1>\n",
        "\n",
        "<author>Julio Waissman Vilanova</author>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/juliowaissman/intro-rn/blob/main/mnist-densa.ipynb\">\n",
        "<img src=\"https://i.ibb.co/2P3SLwK/colab.png\" width=30pt />\n",
        "<i>Para usar en Google Colab</i></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRNe_hP9Irck"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Vamos a hacer nuestra primera red neuronal en lo que se considera el *Hola Mundo* de las redes neuronales: el conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/). El conjunto de datos MNIST consiste en 60,000 imagenes en blanco y negro de entrenamiento y 10,000 de prueba. Todas las imagenes son sobre dígitos escritos a mano. Así, la salida de la red neuronal debería ser la probabilidad de preteneces a 10 clases diferentes: los dígitos de 0 a 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RkiuEgHWIOBL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShMUhJg24vqO"
      },
      "source": [
        "Vamos bajando el conjunto de datos y revisando unas imágenes. El modulo [`tf.keras.dataset`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) viene con algunos conjuntos muy populares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjqs-PO341X0"
      },
      "outputs": [],
      "source": [
        "# TensorFlow ya te permite bajar ciertos conjuntos de datos famosos\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\n",
        "train_labels = (train_labels).astype(np.int64)\n",
        "test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\n",
        "test_labels = (test_labels).astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51uWgdTC5ts8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "random_inds = np.random.choice(60000,64)\n",
        "for i in range(64):\n",
        "    plt.subplot(8,8,i+1)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.title(train_labels[image_ind], y=.8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZJdqEF77U_s"
      },
      "source": [
        "## Diseño de una red neuronal densa\n",
        "\n",
        "\n",
        "Construiremos una red neuronal simple que consta de dos capas completamente conectadas y la aplicaremos a la tarea de clasificación de dígitos. En última instancia, nuestra red generará una distribución de probabilidad entre las clases de 10 dígitos (0-9). Esta primera arquitectura que construiremos se muestra a continuación:\n",
        "\n",
        "![](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab2/img/mnist_2layers_arch.png \"Arquitectura CNN para clasificación MNIST\")\n",
        "\n",
        "Para definir la arquitectura de esta red neuronal completamente conectada, usaremos la API de Keras y definiremos el modelo usando la clase [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras /modelos/Secuencial).\n",
        "\n",
        "Usamos primero una capa [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten), que aplana la entrada para que pueda introducirse en el modelo. Las capas densas, en la primera usaremos una activación no lineal conocida como `ReLU` y para la salida, como queremos una distribución de probabilidad, usaremos una activación tipo `softmax`.\n",
        "\n",
        "En el siguiente bloque, definirá las capas completamente conectadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3piWIRE78Oxd"
      },
      "outputs": [],
      "source": [
        "def build_fc_model():\n",
        "  fc_model = tf.keras.Sequential([\n",
        "      # Capa tipo Faltten para aplanar las imágenes\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      # '''TODO: Capa oculta completamente conectada.'''\n",
        "      tf.keras.layers.Dense(128, activation= #'''TODO'''),\n",
        "\n",
        "      # '''TODO: Define la capa de salida con las probabilidades por clase'''\n",
        "      #[TODO Capa densa con las probabilidades de salida]\n",
        "\n",
        "  ])\n",
        "  return fc_model\n",
        "\n",
        "model = build_fc_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dQ_m6fb9Je4"
      },
      "source": [
        "A medida que avancemos en la siguiente parte, es posible que desees realizar cambios en la arquitectura definida anteriormente. **Ten en cuenta que para actualizar el modelo más adelante, deberás volver a ejecutar la celda anterior para reinicializar el modelo.**\n",
        "\n",
        "Pensemos en la red que acabamos de crear. La primera capa de esta red, `tf.keras.layers.Flatten`, transforma el formato de las imágenes de una matriz 2D (28 x 28 píxeles) a un vector 1D de 28 * 28 = 784 entradas. Puede pensar en esta capa como desapilar filas de píxeles en la imagen y alinearlas. No hay parámetros aprendidos en esta capa; solo reformatea los datos.\n",
        "\n",
        "Una vez aplanados los píxeles, la red consta de una secuencia de dos capas `tf.keras.layers.Dense`. Estas son capas neuronales completamente conectadas. La primera capa \"densa\" tiene 128 nodos (o neuronas). La segunda capa debe devolver una serie de puntuaciones de probabilidad que suman 1. Cada nodo contiene una puntuación que indica la probabilidad de que la imagen actual pertenezca a una de las clases de dígitos escritos a mano.\n",
        "\n",
        "Eso define nuestro modelo totalmente conectado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_4Ugv7e949G"
      },
      "source": [
        "## Compila el modelo\n",
        "\n",
        "Antes de entrenar el modelo, necesitamos definir algunas configuraciones más. Estos se agregan durante el paso [`compile`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#compile) del modelo:\n",
        "\n",
        "* *Función de pérdida*: define cómo medimos la precisión del modelo durante el entrenamiento. Durante el entrenamiento queremos minimizar esta función, lo que \"dirigirá\" el modelo en la dirección correcta.\n",
        "* *Optimizador*: define cómo se actualiza el modelo en función de los datos que ve y su función de pérdida.\n",
        "* *Métricas*: aquí podemos definir las métricas utilizadas para monitorear los pasos de capacitación y prueba. En este ejemplo, veremos *accuracy*, la fracción de imágenes que están clasificadas correctamente.\n",
        "\n",
        "Comenzaremos utilizando un optimizador de descenso de gradiente estocástico (SGD) inicializado con una tasa de aprendizaje de 0.1. Dado que estamos realizando una tarea de clasificación categórica, querremos utilizar la [pérdida de entropía cruzada](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy).\n",
        "\n",
        "Mas adelante es posible que quieras experimentar tanto con la elección del optimizador como con la tasa de aprendizaje y evaluar cómo afectan la precisión del modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0bgh7KC8-fD4"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlydy8CZ_A1m"
      },
      "source": [
        "## Entrena el modelo\n",
        "\n",
        "Ahora estamos listos para entrenar nuestro modelo, lo que implicará introducir los datos de entrenamiento (\"train_images\" y \"train_labels\") en el modelo y luego pedirle que aprenda las asociaciones entre imágenes y etiquetas. También necesitaremos definir el tamaño del lote y la cantidad de épocas, o iteraciones sobre el conjunto de datos MNIST, que se usarán durante el entrenamiento.\n",
        "\n",
        "En la libreta pasada, vimos cómo podemos usar `GradientTape` para optimizar las pérdidas y entrenar modelos con descenso de gradiente estocástico a pie.\n",
        "\n",
        "Con Keras, después de definir la configuración del modelo y el optimizador en el paso `compilar`, también podemos realizar el entrenamiento llamando al metodo [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#fit). Usaremos esto para entrenar nuestro modelo totalmente conectado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymH5PlpH_mJE"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvpbMTfI_1nX"
      },
      "source": [
        "A medida que el modelo se entrena, se muestran las métricas de pérdida y precisión. Con cinco épocas y una tasa de aprendizaje de 0.01, este modelo completamente conectado debería alcanzar una precisión de aproximadamente 0.97 (o 97%) en los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNLyHq4SAH8K"
      },
      "source": [
        "## Evaluar la calidad del modelo entrenado\n",
        "\n",
        "Ahora que hemos entrenado el modelo, podemos pedirle que haga predicciones sobre un conjunto de pruebas que no haya visto antes. En este ejemplo, la matriz `test_images` comprende nuestro conjunto de datos de prueba. Para evaluar la precisión, podemos verificar si las predicciones del modelo coinciden con las etiquetas de `test_labels`.\n",
        "\n",
        "Vamos a utilizar el método [`evaluate`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#evaluate) para evaluar el modelo en el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoctqwigAf8K"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(\n",
        "    test_images,\n",
        "    test_labels\n",
        ")\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOjDkyl8A7tU"
      },
      "source": [
        "Puedes observar que la precisión del conjunto de datos de prueba es un poco menor que la precisión del conjunto de datos de entrenamiento.\n",
        "\n",
        "Si esta brecha entre la precisión del entrenamiento y la precisión de las pruebas es muy grande, entonces hay un *sobreajuste*, que es cuando un modelo de aprendizaje automático funciona peor con datos nuevos que con sus datos de entrenamiento. Así, aunque el modelo parezca bueno, es poco confiable en nuevos datos, que es para lo que lo queremos en ñultima instancia.\n",
        "\n",
        "**¿Cuál es la mayor precisión que puede lograr con este primer modelo totalmente conectado?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hujOUolrB4Hp"
      },
      "source": [
        "## Otra base de datos más compleja\n",
        "\n",
        "Debido a que MNIST es un conjunto relativamente muy sencillo de ajustar, hay otros conjuntos propuestos ligeramente más complicados de solucionar, entre los que se encuentre el [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) que cambia los dígitos a mano por cosas ligeramiente más complicadas como tipos de ropa, zapatos y accesorios.\n",
        "\n",
        "El tamaño de las imágenes, el número de clases, así como el número de ejemplos de entrenamiento y aprendizaje son igualitos a los de MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJDZTrizDTcu"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\n",
        "train_labels = (train_labels).astype(np.int64)\n",
        "test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\n",
        "test_labels = (test_labels).astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2NaNdN0DlWd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "random_inds = np.random.choice(60000,64)\n",
        "for i in range(64):\n",
        "    plt.subplot(8,8,i+1)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.title(train_labels[image_ind], y=.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ3zXWAsD9Z3"
      },
      "source": [
        "## Taller\n",
        "\n",
        "**Diseña una red neuronal densa, selecciona los hiperparámetros y entrenala para el conjunto de datos de Fashion MNIST.**\n",
        "\n",
        "**Tienes 15 minutos, y vamos a ver quien logra el mayor *accuracy* en el conjunto de test, y con el menor sobreajuste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CMJUoVs5Eecc"
      },
      "outputs": [],
      "source": [
        "def build_fashon_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      # Capa tipo Faltten para aplanar las imágenes\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      # '''TODO: Agrega cuantas capas y unidades quieras'''\n",
        "      #[TODO Capa(s) densas oculta(s)],\n",
        "\n",
        "      # '''TODO: Define la capa de salida con las probabilidades por clase'''\n",
        "      #[TODO Capa densa con las probabilidades de salida]\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_fashon_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nAYSgmZNEthD"
      },
      "outputs": [],
      "source": [
        "# Podrias cambiar el optimizador o la tasa de aprendizaje\n",
        "lr = 1e-1\n",
        "optimizador = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizador,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6FFYFhRE0jb"
      },
      "outputs": [],
      "source": [
        "# Puedes probar con el tamaño de batch o los epochs\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWwC5HGFDdP"
      },
      "outputs": [],
      "source": [
        "# Aquí vamos a ver que tan ben funciona\n",
        "test_loss, test_acc = model.evaluate(\n",
        "    test_images,\n",
        "    test_labels\n",
        ")\n",
        "print('Test accuracy:', test_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
